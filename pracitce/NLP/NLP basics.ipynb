{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Your all predicament are inherent in me. Do you understand me?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To tokenizing the the senteces in the document\n",
    "        It basically separates the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your all predicament are inherent in me.', 'Do you understand me?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing the sample_text\n",
    "\n",
    "    Tokenizing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the stop word in \"english\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "#stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the punctuations to the stop words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation = list(string.punctuation)\n",
    "stop += punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the context of the word.... \n",
    "        It tells whether the particular word is Noun, verb, adjective and averb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 'NN')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "pos = pos_tag(word_tokenize(\"r\"))\n",
    "pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning our document.\n",
    "Removing the stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Your', 'predicament', 'inherent', 'Do', 'understand']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_words = [w for w in words if not w in stop]\n",
    "clean_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming Basics\n",
    "Stemming is the process of converting the words to their root words.\n",
    "Suppose there are words \n",
    "\"play\" \"playing\" \"played\"\n",
    "so if these are converted to \"play\"\n",
    "\n",
    "But if there is \"player\" then it shouldn't be converted.\n",
    "It kind of doesn't makes sense. player is someone playing. context is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['play', 'play', 'pali', 'player', 'happi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem_words = [\"play\",\"playing\",\"palyed\",\"player\",\"happying\"]\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(w) for w in stem_words]\n",
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing \n",
    "Here we convert the words to their root words based on their part of speech.\n",
    "Why does it requires Part Of Speech? So suppose there is word \"painting\"\n",
    "In this word it can mean \"I am Painting\" or \"This is a painting\" Now in this second sentence \"painting\" acts as NOUN so we don't want it to be lemmetized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paint'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize(\"painting\",pos=\"v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with a movie Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#movie_reviews.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(movie_reviews.fileids('neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['capsule', ':', 'in', '2176', 'on', 'the', 'planet', ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_reviews.words(movie_reviews.fileids()[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for category in movie_reviews.categories():\n",
    "    for fileid in movie_reviews.fileids(category):\n",
    "        documents.append(((movie_reviews.words(fileid)),category))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['plot', ':', 'two', 'teen', 'couples', 'go', 'to', ...], 'neg'),\n",
       " (['the', 'happy', 'bastard', \"'\", 's', 'quick', 'movie', ...], 'neg'),\n",
       " (['it', 'is', 'movies', 'like', 'these', 'that', 'make', ...], 'neg'),\n",
       " (['\"', 'quest', 'for', 'camelot', '\"', 'is', 'warner', ...], 'neg'),\n",
       " (['synopsis', ':', 'a', 'mentally', 'unstable', 'man', ...], 'neg')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As documents are categorised.\n",
    "So to split the document for training and testing.\n",
    "We will shuffle our data as currently it is sorted according to the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['years', 'ago', ',', 'robin', 'williams', 'made', ...], 'neg'),\n",
       " (['one', 'of', 'my', 'brother', \"'\", 's', 'favorite', ...], 'neg'),\n",
       " (['leonardo', 'decaprio', '(', 'what', \"'\", 's', ...], 'pos'),\n",
       " (['synopsis', ':', 'retiring', 'detective', 'jerry', ...], 'pos'),\n",
       " (['movie', 'reviewers', 'have', 'an', 'obligation', ...], 'neg')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random \n",
    "random.shuffle(documents)\n",
    "documents[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So below is the function used to clean the reviws.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatizing our data\n",
    "So lemmatizing is basically trying to figure out the root word for our various words.\n",
    "for e.g.-> \"better\" becomes \"good\"\n",
    "            \"painting\" becomes \"paint\" or remains \"painting\" depending upon the pos - (part of speech).\n",
    "            so if pos of \"painting\" is noun denoted by \"n\" then it won't change if \"verb\" then it will."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "## i forgot the '()' and it cost me one complete day....\n",
    "# Lesson learned () it is very important\n",
    "lemma = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert the value returned by pos_tag() to suit the lemmatizer parameter format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def get_simple_pos(pos):\n",
    "    if pos.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif pos.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif pos.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif pos.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else :\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why I love Jupyter Notebook -;\n",
    "So before applying what we have learned in a function\n",
    "Below box is simple test that everything works fine as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hug'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"hugging\"\n",
    "pos = pos_tag([word])\n",
    "pos = get_simple_pos(pos[0][1])\n",
    "#word = lemma.lemmatize( self = w, pos = get_simple_pos(pos[0][1]))\n",
    "word = lemma.lemmatize(word,pos = pos)\n",
    "word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words \n",
    "\n",
    "As these words don't add any meaning to our documents.\n",
    "These are present irrespective of the class our document belongs.\n",
    "So its better to remove them...\n",
    "Rule of life-> You don't contribute, see yourelf fade away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "stops = set(stopwords.words('english'))\n",
    "punct = list(string.punctuation)\n",
    "stops.update(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to remove the stop word and lemmatize each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_review(words):\n",
    "    output_words = []\n",
    "    for w in words:\n",
    "        if w.lower() not in stops:\n",
    "            pos = pos_tag([w])\n",
    "            pos = get_simple_pos(pos[0][1])\n",
    "            lemmatized_word = lemma.lemmatize(w,pos )\n",
    "            output_words.append(lemmatized_word.lower())\n",
    "    return output_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why this comment?**<br>\n",
    ">   1-> I executed the below function<br>\n",
    "    2-> Went downstairs 2 floors<br>\n",
    "    3-> Filled water bottle<br>\n",
    "    4-> Lost the cap<br>\n",
    "    5-> Found the cap of bottle<br>\n",
    "    6-> Came back to working area<br>\n",
    "    7-> Then it got completed.<br>\n",
    "Seems like it was waiting for his master's return. ( <font color = blue>Dobby is free now -;</font>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **See here** __[basics](https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed)__ **of Markdown**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = [(clean_review(document),category) for document ,category in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check whether it worked or not\n",
    "#document[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the Training Data and Testing Data\n",
    "    We cannot use whole data to create feature. \n",
    "    And then use the same data to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = document[0:1500]\n",
    "testing_data = document[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#documents[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Creation\n",
    "So **'document'** is list of (list,category) <br>\n",
    "Think in this way suppose you have 2-D array.<br>\n",
    "Now each row has two columns<br>1-column has list of all the lemmatized words of one document.<br>2-column Represents the category it belongs to, here 'pos' or 'neg'\n",
    "\n",
    "To create we will use all the words in all the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in training_data:\n",
    "    all_words +=i[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now your **\"all_words\"** contains all the words which have ever occured in training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "530862"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of all the words present in total of training document.\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency Distribution of the words\n",
    "And a method to know the most_common words in our documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(all_words)\n",
    "freq_dist = freq.most_common(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept of Feature Generation\n",
    "    We will use n most_common feature to build our feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = []\n",
    "feature=[ i[0] for i in freq_dist]\n",
    "#feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It will return a dictionary.\n",
    "Format of dictionary will be -> dictionary{ **word_1** : True, **Word_2** : False, **Word_3** : True, ..... **Word_n** : True }\n",
    "\n",
    "Format of the Feature_set will be a Dictionary having many dictionary.\n",
    "So when any algorithm uses this feature set...<br>Basically it have n*m kind of matrices, where **column_name** are all the possible words. <br> <br>Each **row** represent a Training_document in each column of each row either **True** or **False**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_dict(word):\n",
    "    cf = {}\n",
    "    #just to remove repeated words\n",
    "    word_set = set(word)\n",
    "    for w in feature:\n",
    "        cf[w] = w in word_set\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Below we are converting the training_documents in the format which is required by \n",
    "### nltk.naive_bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_documents = []\n",
    "training_documents = [(get_feature_dict(doc),category) for doc,category in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_documents = []\n",
    "testing_documents = [(get_feature_dict(doc),category) for doc,category in testing_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_set[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = NaiveBayesClassifier.train(training_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.842"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.classify.accuracy(classifier, testing_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the name itself below tells which all features are more important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             outstanding = True              pos : neg    =      9.2 : 1.0\n",
      "                   mulan = True              pos : neg    =      8.3 : 1.0\n",
      "             wonderfully = True              pos : neg    =      7.8 : 1.0\n",
      "                  seagal = True              neg : pos    =      6.6 : 1.0\n",
      "              ridiculous = True              neg : pos    =      6.5 : 1.0\n",
      "                   awful = True              neg : pos    =      5.5 : 1.0\n",
      "                   waste = True              neg : pos    =      5.2 : 1.0\n",
      "                  poorly = True              neg : pos    =      5.0 : 1.0\n",
      "                lebowski = True              pos : neg    =      5.0 : 1.0\n",
      "                   badly = True              neg : pos    =      4.8 : 1.0\n",
      "                    jedi = True              pos : neg    =      4.7 : 1.0\n",
      "                    lame = True              neg : pos    =      4.6 : 1.0\n",
      "              friendship = True              pos : neg    =      4.5 : 1.0\n",
      "                   bland = True              neg : pos    =      4.2 : 1.0\n",
      "                  allows = True              pos : neg    =      4.2 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will try to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import svc \n",
    "from nltk.classify.scikitlearn import SklearnClassifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
